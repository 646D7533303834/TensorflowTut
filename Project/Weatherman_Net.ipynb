{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directory where summaries will be stored\n",
    "LOG_DIR = './logs'\n",
    "# unrolled through 48 time steps\n",
    "TIME_STEPS = 48\n",
    "# number of inputs\n",
    "FEATURE_COUNT = 11\n",
    "TRAINING_STEPS = 10000\n",
    "BATCH_SIZE = 60\n",
    "# hidden LSTM units\n",
    "NUM_UNITS = 60\n",
    "# learning rate for adam\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_data(data, time_steps, labels=False):\n",
    "    \"\"\"\n",
    "    creates new data frame based on previous observation\n",
    "      * example:\n",
    "        l = [1, 2, 3, 4, 5]\n",
    "        time_steps = 2\n",
    "        -> labels == False [[1, 2], [2, 3], [3, 4]]\n",
    "        -> labels == True [2, 3, 4, 5]\n",
    "    \"\"\"\n",
    "    rnn_df = []\n",
    "    for i in range(len(data) - time_steps - 1):\n",
    "        if labels:\n",
    "            data_ = data.iloc[i + time_steps].as_matrix()\n",
    "        else:\n",
    "            data_ = data.iloc[i: i + time_steps].as_matrix()\n",
    "\n",
    "        rnn_df.append(data_)\n",
    "    return np.array(rnn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, val_size=0.1, test_size=0.1):\n",
    "    \"\"\"\n",
    "    splits data to training, validation and testing parts\n",
    "    \"\"\"\n",
    "    ntest = int(round(len(data) * (1 - test_size)))\n",
    "    nval = int(round(len(data.iloc[:ntest]) * (1 - val_size)))\n",
    "\n",
    "    df_train, df_val, df_test = data.iloc[:nval], data.iloc[nval:ntest], data.iloc[ntest:]\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, time_steps, labels=False, val_size=0.15, test_size=0.15):\n",
    "    \"\"\"\n",
    "    Given the number of `time_steps` and some data,\n",
    "    prepares training, validation and test data for an lstm cell.\n",
    "    \"\"\"\n",
    "    df_train, df_val, df_test = split_data(data, val_size, test_size)\n",
    "    return (rnn_data(df_train, time_steps, labels=labels),\n",
    "            rnn_data(df_val, time_steps, labels=labels),\n",
    "            rnn_data(df_test, time_steps, labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_normalize(x, define_max=0, define_min=999):\n",
    "    if define_min == 999:\n",
    "        norm_min = x.min()\n",
    "    else:\n",
    "        norm_min = define_min\n",
    "    norm_min = x.min()\n",
    "    if define_max == 0:\n",
    "        max = x.max()\n",
    "    else:\n",
    "        max = define_max\n",
    "    diff = max - norm_min\n",
    "\n",
    "    return ((x - norm_min) / diff) - 0.5, norm_min, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_denormalize(x, denorm_min, diff):\n",
    "    return ((x + 0.5) * diff) + denorm_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create log directory\n",
    "if tf.gfile.Exists(LOG_DIR):\n",
    "    tf.gfile.DeleteRecursively(LOG_DIR)\n",
    "tf.gfile.MakeDirs(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "# Expects the file to to be in folder named 'data' which resides in\n",
    "# the folder that this notebook resides in\n",
    "file_path = 'data'\n",
    "file_name = '/1114147.csv'\n",
    "df_raw_data = pd.read_csv(file_path + file_name, encoding=\"ISO-8859-1\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first two columns\n",
    "df_inputs = df_raw_data.iloc[:, 2:]\n",
    "# Change date to a datetime object so that it is easier to manipulate\n",
    "df_inputs.DATE = pd.to_datetime(df_inputs.DATE)\n",
    "\n",
    "# Create individual columns for seperate date values\n",
    "df_inputs['year'] = df_inputs.DATE.dt.year\n",
    "df_inputs['month'] = df_inputs.DATE.dt.month\n",
    "df_inputs['day'] = df_inputs.DATE.dt.dayofyear\n",
    "df_inputs['hour'] = df_inputs.DATE.dt.hour\n",
    "\n",
    "# Blank cells of Columns we will use with zeros\n",
    "df_inputs.HOURLYWindGustSpeed = df_inputs.HOURLYWindGustSpeed.fillna(0)\n",
    "df_inputs.HOURLYPrecip = df_inputs.HOURLYPrecip.fillna(0)\n",
    "\n",
    "# Get a subset of the weather data that corresponds to what is available\n",
    "# daily without a request\n",
    "df_inputs = df_inputs[[\n",
    "    'day',\n",
    "    'hour',\n",
    "    'HOURLYWindDirection',\n",
    "    'HOURLYWindGustSpeed',\n",
    "    'HOURLYVISIBILITY',\n",
    "    'HOURLYDRYBULBTEMPF',\n",
    "    'HOURLYDewPointTempF',\n",
    "    'HOURLYRelativeHumidity',\n",
    "    'HOURLYAltimeterSetting',\n",
    "    'HOURLYSeaLevelPressure',\n",
    "    'HOURLYPrecip'\n",
    "]]\n",
    "\n",
    "# Drop the rows without a complet eset of data\n",
    "df_inputs = df_inputs.dropna()\n",
    "\n",
    "# Remove extranious non digit data\n",
    "df_inputs.HOURLYWindDirection.replace('VRB', -1, inplace=True)\n",
    "df_inputs.HOURLYVISIBILITY.replace(['V', 's'], '', regex=True, inplace=True)\n",
    "df_inputs.HOURLYDRYBULBTEMPF.replace(['V', 's'], '', regex=True, inplace=True)\n",
    "df_inputs.HOURLYDewPointTempF.replace(['V', 's'], '', regex=True, inplace=True)\n",
    "df_inputs.HOURLYAltimeterSetting.replace(['V', 's'], '', regex=True, inplace=True)\n",
    "df_inputs.HOURLYSeaLevelPressure.replace(['V', 's'], '', regex=True, inplace=True)\n",
    "df_inputs.HOURLYPrecip.replace(['T', 's'], [0.001, ''], regex=True, inplace=True)\n",
    "\n",
    "# change all values from strings to numbers\n",
    "df_inputs = df_inputs.apply(pd.to_numeric)\n",
    "df_inputs = df_inputs.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data to make it easier for Tensorflow to process\n",
    "df_inputs.hour, hour_min, hour_diff = generic_normalize(df_inputs.hour)\n",
    "df_inputs.day, day_min, day_diff = generic_normalize(df_inputs.day)\n",
    "df_inputs.HOURLYWindDirection, HOURLYWindDirection_min, HOURLYWindDirection_diff = \\\n",
    "    generic_normalize(df_inputs.HOURLYWindDirection)\n",
    "df_inputs.HOURLYWindGustSpeed, HOURLYWindGustSpeed_min, HOURLYWindGustSpeed_diff = \\\n",
    "    generic_normalize(df_inputs.HOURLYWindGustSpeed)\n",
    "df_inputs.HOURLYVISIBILITY, HOURLYVISIBILITY_min, HOURLYVISIBILITY_diff = \\\n",
    "    generic_normalize(df_inputs.HOURLYVISIBILITY)\n",
    "df_inputs.HOURLYDRYBULBTEMPF, HOURLYDRYBULBTEMPF_min, HOURLYDRYBULBTEMPF_diff = \\\n",
    "    generic_normalize(df_inputs.HOURLYDRYBULBTEMPF)\n",
    "df_inputs.HOURLYDewPointTempF, HOURLYDewPointTempF_min, HOURLYDewPointTempF_diff = \\\n",
    "    generic_normalize(df_inputs.HOURLYDewPointTempF)\n",
    "df_inputs.HOURLYRelativeHumidity, HOURLYRelativeHumidity_min, HOURLYRelativeHumidity_diff = \\\n",
    "    generic_normalize(df_inputs.HOURLYRelativeHumidity)\n",
    "df_inputs.HOURLYAltimeterSetting, HOURLYAltimeterSetting_min, HOURLYAltimeterSetting_diff = \\\n",
    "    generic_normalize(df_inputs.HOURLYAltimeterSetting)\n",
    "df_inputs.HOURLYSeaLevelPressure, HOURLYSeaLevelPressure_min, HOURLYSeaLevelPressure_diff = \\\n",
    "    generic_normalize(df_inputs.HOURLYSeaLevelPressure)\n",
    "df_inputs.HOURLYPrecip, HOURLYPrecip_min, HOURLYPrecip_diff = \\\n",
    "    generic_normalize(df_inputs.HOURLYPrecip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for Tensorflow\n",
    "train_x, val_x, test_x = prepare_data(df_inputs, TIME_STEPS)\n",
    "train_y, val_y, test_y = prepare_data(df_inputs, TIME_STEPS, labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining placeholders\n",
    "# input data placeholder\n",
    "x = tf.placeholder(\"float\", [None, TIME_STEPS, FEATURE_COUNT])\n",
    "# input label placeholder\n",
    "y = tf.placeholder(\"float\", [None, FEATURE_COUNT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing the input tensor from [batch_size,n_steps,n_input] to\n",
    "# \"time_steps\" number of [batch_size,n_input] tensors\n",
    "input = tf.unstack(x, TIME_STEPS, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "# two LSTM layers with layer normalization\n",
    "with tf.variable_scope(\"rnn1\"):\n",
    "    lstm_layer = rnn.LayerNormBasicLSTMCell(NUM_UNITS, forget_bias=1)\n",
    "    outputs, _ = rnn.static_rnn(lstm_layer, input, dtype=\"float32\")\n",
    "with tf.variable_scope(\"rnn2\"):\n",
    "    # you can find a batch norm cell online\n",
    "    lstm_layer2 = rnn.LayerNormBasicLSTMCell(NUM_UNITS, forget_bias=1)\n",
    "    outputs, _ = rnn.static_rnn(lstm_layer2, outputs, dtype=\"float32\")\n",
    "# fully connected layer to produce the desired outputs\n",
    "with tf.variable_scope(\"fc1\"):\n",
    "    # definately use xavier init\n",
    "    # weights and biases of appropriate shape to accomplish above task\n",
    "    out_weights = tf.get_variable(\"out_weights\", shape=[NUM_UNITS, FEATURE_COUNT],\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "    out_bias = tf.get_variable(\"out_bias\", shape=[NUM_UNITS, FEATURE_COUNT],\n",
    "                               initializer=tf.contrib.layers.xavier_initializer())\n",
    "    prediction = tf.matmul(outputs[-1], out_weights) + out_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "with tf.name_scope(\"loss_function\") as scope:\n",
    "    loss = tf.reduce_mean(tf.nn.l2_loss(prediction - y))\n",
    "    tf.summary.scalar(\"loss_function\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize variables\n",
    "init_g = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all the summaries for Tensorboard\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create shuffleable dataset that we can batch sample from\n",
    "c_t = np.c_[train_x.reshape(len(train_x), -1), train_y.reshape(len(train_y), -1)]\n",
    "x_t = c_t[:, :train_x.size // len(train_x)].reshape(train_x.shape)\n",
    "y_t = c_t[:, :train_y.size // len(train_y)].reshape(train_y.shape)\n",
    "\n",
    "# Validation data\n",
    "c_v = np.c_[val_x.reshape(len(val_x), -1), val_y.reshape(len(val_y), -1)]\n",
    "x_v = c_v[:, :val_x.size // len(val_x)].reshape(val_x.shape)\n",
    "y_v = c_v[:, :val_y.size // len(val_y)].reshape(val_y.shape)\n",
    "\n",
    "# Testing data\n",
    "c_tt = np.c_[test_x.reshape(len(test_x), -1), test_y.reshape(len(test_y), -1)]\n",
    "x_tt = c_tt[:, :test_x.size // len(test_x)].reshape(test_x.shape)\n",
    "y_tt = c_tt[:, :test_y.size // len(test_y)].reshape(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_g)\n",
    "    sess.run(init_l)\n",
    "    summary_writer = tf.summary.FileWriter(LOG_DIR, graph=sess.graph)\n",
    "    iter = 1\n",
    "    while iter < TRAINING_STEPS:\n",
    "        # Shuffle data\n",
    "        np.random.shuffle(c_t)\n",
    "        batch_x = x_t[:BATCH_SIZE]\n",
    "        batch_y = y_t[:BATCH_SIZE]\n",
    "        np.random.shuffle(c_v)\n",
    "        batch_xv = x_v[:BATCH_SIZE]\n",
    "        batch_yv = y_v[:BATCH_SIZE]\n",
    "        np.random.shuffle(c_tt)\n",
    "        batch_xt = x_tt[:BATCH_SIZE]\n",
    "        batch_yt = y_tt[:BATCH_SIZE]\n",
    "        \n",
    "        # Optimize the model\n",
    "        sess.run(opt, feed_dict={x: batch_x, y: batch_y})\n",
    "        \n",
    "        # Get summary data points for analysis in Tensorboard\n",
    "        if iter % 10 == 0:\n",
    "            \n",
    "            los = sess.run(loss, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        summary_str = sess.run(merged_summary_op, feed_dict={x: batch_x, y: batch_y})\n",
    "        summary_writer.add_summary(summary_str, iter)\n",
    "        iter = iter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_subset = df_raw_data.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_subset.DATE = pd.to_datetime(df_num_subset.DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_subset.DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to put the years more middle of the road here\n",
    "df_num_subset['year_norm'] = (df_num_subset.DATE.dt.year - 2000) / 20\n",
    "df_num_subset['month_norm'] = df_num_subset.DATE.dt.month / 12\n",
    "df_num_subset['day_norm'] = df_num_subset.DATE.dt.day / 31\n",
    "df_num_subset['time_norm'] = ((df_num_subset.DATE.dt.hour * 60) + df_num_subset.DATE.dt.minute) / 1440\n",
    "df_num_subset['time_norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_subset.LATITUDE = df_num_subset.LATITUDE / 90\n",
    "df_num_subset.LATITUDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_subset.LONGITUDE = df_num_subset.LONGITUDE / 180\n",
    "df_num_subset.LONGITUDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_subset.REPORTTPYE.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir=/tmp/tensorflow/mnist/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_subset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_raw_data.drop(['STATION', 'STATION_NAME', 'REPORTTPYE', 'HOURLYSKYCONDITIONS', 'HOURLYPRSENTWEATHERTYPE',\n",
    "                        'HOURLYPressureTendency', 'HOURLYPressureChange',\n",
    "                        'DAILYMaximumDryBulbTemp', 'DAILYMinimumDryBulbTemp', 'DAILYAverageDryBulbTemp',\n",
    "                        'DAILYDeptFromNormalAverageTemp', 'DAILYAverageRelativeHumidity', 'DAILYAverageDewPointTemp',\n",
    "                        'DAILYAverageWetBulbTemp', 'DAILYHeatingDegreeDays', 'DAILYCoolingDegreeDays'], axis=1)\n",
    "df2 = df2.iloc[:, :21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.HOURLYWindGustSpeed = df2.HOURLYWindGustSpeed.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.DATE = pd.to_datetime(df2.DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.to_datetime(df3.DATE)\n",
    "x = x.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.asfreq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to put the years more middle of the road here\n",
    "df3.DATE = pd.to_datetime(df3.DATE)\n",
    "df3['year'] = df3.DATE.dt.year\n",
    "df3['month'] = df3.DATE.dt.month\n",
    "df3['day'] = df3.DATE.dt.day\n",
    "df3['hour'] = df3.DATE.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_go_time = df3[[\n",
    "    'year', \n",
    "    'month',\n",
    "    'day',\n",
    "    'hour',\n",
    "    'HOURLYWindDirection',\n",
    "    'HOURLYWindGustSpeed',\n",
    "    'HOURLYVISIBILITY',\n",
    "    'HOURLYDRYBULBTEMPF',\n",
    "    'HOURLYWETBULBTEMPF',\n",
    "    'HOURLYDewPointTempF',\n",
    "    'HOURLYRelativeHumidity',\n",
    "    'HOURLYAltimeterSetting',\n",
    "    'HOURLYSeaLevelPressure']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_go_time.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_go_time.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
